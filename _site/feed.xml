<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.10.0">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2025-12-02T22:05:48+05:30</updated><id>http://localhost:4000/feed.xml</id><title type="html">The Intuitive Machine</title><subtitle>Thoughts and Stories</subtitle><author><name>LuaGeeko</name></author><entry><title type="html">Ben discovers the Omnitrix of Uncertainty: Intro to Probability &amp;amp; Random Variables</title><link href="http://localhost:4000/2025/11/19/Ben-discovers-the-Omnitrix-of-Uncertainity.html" rel="alternate" type="text/html" title="Ben discovers the Omnitrix of Uncertainty: Intro to Probability &amp;amp; Random Variables" /><published>2025-11-19T15:30:00+05:30</published><updated>2025-11-19T15:30:00+05:30</updated><id>http://localhost:4000/2025/11/19/Ben-discovers-the-Omnitrix-of-Uncertainity</id><content type="html" xml:base="http://localhost:4000/2025/11/19/Ben-discovers-the-Omnitrix-of-Uncertainity.html"><![CDATA[<p><img src="/assets/images/probability/20251119_2050_ben_header.png" alt="Omnitrix Header Banner" class="header-image" /></p>

<p>Probablitity helps us <em>quantify uncertainity before something happens</em>. In our uncertain world, our daily decisions often rely on estimating how likely an event is to occur or not. Unexpected situations pop up out of nowhere-just like Ben on road trip with Grandpa and his sister Gwen. While wandering in forest, spots what seems like a shooting star, but instead it crashes nearby. Inside the crater lies a strange capsule containing a cool and glowing watch that latches onto Ben the moment he reaches for it: <strong>the Omnitrix</strong></p>

<h2 id="sample-space">Sample Space</h2>

<p>Now that the Omnitrix is on his wrist, he wanted to experiment and pushes the core down and <em>boom</em>-he transforms into <strong>Heatblast</strong>. Lets understand this from perspective of probability terms, imagine the Omnitrix as holding onto a <code class="language-plaintext highlighter-rouge">sample space</code>: a complete set of <strong>all possible outcomes</strong> that can occur when Ben activates it.</p>

<p><strong>The Omnitrix‚Äôs contains full catalog of aliens == <em>SAMPLE SPACE</em></strong></p>

<h2 id="random-variable-rv">Random Variable (R.V)</h2>

<p>Now when he pressed the Omnitrix (think this as one experiment), to try out, transformation to one of the alien can be thought as <strong>one outcome</strong> from the sample space.</p>

<p>But how do I quantify my outcomes, say I want to measure or compare one outcome to another, I need some numerical number attached to it. For example in Ben‚Äôs case, outcomes like ‚ÄúHeatblast‚Äù, ‚ÄúFour Arms‚Äù, ‚ÄúDiamondhead‚Äù are not numbers. I need some function-a mapping- that assigns a <strong>real number</strong> to each outcome.</p>

\[A\ \text{random variable is a function } 
X : \Omega \rightarrow \mathbb{R}\]

\[X(\omega) = \text{a real number for each outcome } \omega \in \Omega.\]

\[X = 
\begin{cases}
1 &amp; \text{if outcome = Heatblast},\\
2 &amp; \text{if outcome = Four Arms},\\
3 &amp; \text{if outcome = Diamondhead},\\
\vdots &amp; 
\end{cases}\]

<h2 id="probability-distribution">Probability Distribution</h2>
<p>Now that we know how the Omnitrix holds a <em>full sample space of alien forms</em>, the next question to ponder on is - <strong>How uncertainity is distributed over all possible values the random variable can take?</strong></p>

<p>In our case <strong>How likely is Ben to transform into each alien when he activates it?</strong> And this brings the concept of <em>probability distribution</em>. A simple way that tells us how probability is assigned to each possible outcome in our sample space.</p>

<p>In Ben‚Äôs world:</p>
<ul>
  <li>Heatblast might appear more often,</li>
  <li>Four Arms might be less frequent,</li>
  <li>Some alien might rarely come out.</li>
</ul>

<p>This gives us a rule that assigns a probability to every outcome. In the next part of the series, we will go a bit deeper and understand <strong>PMF</strong> - for discrete outcomes like alien forms, <strong>PDF</strong> - for continuous cases and CDF.</p>

<p>¬© 2025 Shruti Verma</p>

<p>If this post helped you, feel free to cite or share it. 
This article is part of the ‚ÄúBen 10 Probability Series.‚Äù 
Feedback, ideas, or corrections? I‚Äôd love to hear from you.</p>

<p>You can reach me here:
‚Ä¢ <a href="hop2work@gmail.com">Email</a>
‚Ä¢ <a href="https://www.linkedin.com/in/shruti31">LinkedIn</a></p>]]></content><author><name>LuaGeeko</name></author><category term="probability" /><category term="random-variables" /><category term="information-theory" /><category term="ben10-analogy" /><summary type="html"><![CDATA[Step inside the Omnitrix and see probability theory come alive through Ben 10]]></summary></entry><entry><title type="html">Matrix Decompositions Explained‚Ää-‚ÄäPart I: Eigen Decomposition</title><link href="http://localhost:4000/2025/11/06/Matrix-Decompositions-Explained.html" rel="alternate" type="text/html" title="Matrix Decompositions Explained‚Ää-‚ÄäPart I: Eigen Decomposition" /><published>2025-11-06T15:30:00+05:30</published><updated>2025-11-06T15:30:00+05:30</updated><id>http://localhost:4000/2025/11/06/Matrix-Decompositions-Explained</id><content type="html" xml:base="http://localhost:4000/2025/11/06/Matrix-Decompositions-Explained.html"><![CDATA[<p><img src="/assets/images/matrix_decom_eigen/decomp_bad_good_cop_header.png" alt="Omnitrix Header Banner" class="header-image" /></p>

<h2 id="ever-wondered-why-we-even-need-matrix-decompositions-and-what-they-mean-intuitively">Ever wondered why we even need matrix decompositions and what they mean intuitively¬†?¬†ü§î</h2>
<p>In general, decomposition means breaking down something in simpler components.For matrices this is important because it can help us break matrix into simpler building blocks, that tell its story as an object‚Ää-‚Ääits <em>hidden structure</em>.</p>

<p><strong>*Not ‚Äúingredients‚Äù but valid representations</strong></p>

<p>Just to be clear it‚Äôs not literally breaking it into ‚Äúingredients‚Äù that originally created the matrix, but mathematically valid ways that can represent the same matrix in terms of simpler pieces, that can be analysed and manipulated as needed.</p>

<p>There are different ways for doing matrix decomposition but in this part we are going to focus on Eigen decompositions. We are going to investigate matrix‚Ää-‚Ääour ‚Äòcriminal‚Äô holding secrets. And our objective is to uncover its hidden structure‚Ää-‚Ää<code class="language-plaintext highlighter-rouge">eigenvalues</code> telling us how much it scales and <code class="language-plaintext highlighter-rouge">eigenvectors</code> the directions that stay unchanged.</p>

<h2 id="the-eigen-interrogation-good-copbad-coproutine-">The Eigen interrogation: Good Cop/Bad Cop¬†Routine üëÆ</h2>

<p>We are going to make matrix confess its natural directions and scaling powers, with good cop and bad cop routine:</p>

<ul>
  <li><strong>Local View (Good Cop)</strong>: test one vector at a time to see if it‚Äôs a special direction the matrix only scales.</li>
  <li><strong>Global View (Bad Cop)</strong>: force the matrix to confess and tell us about its‚Ää-‚Ääall eigenvalues and eigenvectors at once‚Ää-‚Ääby doing <code class="language-plaintext highlighter-rouge">Matrix Diagonalization</code>.</li>
</ul>

<p>Before interrogating our <code class="language-plaintext highlighter-rouge">criminal</code> matrix, let me tell you - its <em>symmetric</em> in nature means he will not lie, though it can hold information. Its up to cops to get all the information out.</p>

<p align="center">
  <img src="/assets/images/matrix_decom_eigen/matrix_decom_eigen_1.png" alt="cops with vector" width="650" />
</p>
<p align="center"><em>Eigen-decomposition: Good Cop (left panel) tests one vector at a time, Bad Cop (right panel) makes the matrix confess everything.</em></p>

<h2 id="local-view">Local View</h2>
<p>Let us take a candidate vector to test but make sure its from the <strong>same space as the martrix</strong>. Ah! a important point we have stumbled upon and needs to be cleared before we move on forward.¬†</p>

<p><strong>Why the ‚Äúsame space‚Äù matters?</strong> A matrix $N$ of size $m \times m$ (<strong>a square matrix</strong>) always maps vectors 
from $\mathbb{R}^m$ back into that same space $\mathbb{R}^m$.</p>

<blockquote>
  <p><em>For eigenvalue/eigenvector testing we need Av=Œªv, which means both sides of the equation must ‚Äúlive‚Äù in the same dimension.</em></p>
</blockquote>

<p>If the vector came from a different space, the outputs will not be comparable and that is why the probe vector (our candidate vector) should belong to the same vector space and satisfy the following equation:</p>

\[\underbrace{A}_{\text{Matrix (the criminal)}} 
\; 
\underbrace{\mathbf{v}}_{\text{Candidate vector (from same space)}} 
= 
\underbrace{\lambda}_{\text{Eigenvalue (stretch or shrink factor)}} 
\; 
\underbrace{\mathbf{v}}_{\text{Same vector direction}}\]

<p>So according to our story think of it this way‚Ää-‚Ää</p>

<p>When the Good Cop brings in a candidate vector for questioning, it must belong to the same ‚Äòcriminal background‚Äô as the matrix. A random innocent bystander (a vector from the wrong space) can‚Äôt reveal anything about the matrix‚Äôs secrets. Only those who are part of the same world‚Ää-‚Ääthe same vector space where the matrix operates‚Ää-‚Ääcan expose whether the matrix preserves their direction or twists them around.</p>

<figure style="text-align:center;">
  <div style="display:flex; justify-content:center; gap:10px;">
    <img src="/assets/images/matrix_decom_eigen/matrix_decom_eigen_2.png" alt="Image 1" width="45%" />
    <img src="/assets/images/matrix_decom_eigen/matrix_decom_eigen_3.png" alt="Image 2" width="45%" />
  </div>
  <figcaption><em>Same Space Matters¬†: "Wrong space‚Ää-‚Ääwrong suspect."(left panel). "Interrogations only work when they're from the same vector space." (right panel).</em></figcaption>
</figure>

<p>Lets try to understand the behaviour of our simple symmetric matrix when its introduced with different candidate vectors:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>

<span class="n">A</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span>
              <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mi">3</span><span class="p">]])</span>
</code></pre></div></div>
<p>Before I mentioned that our symmetric criminal is truthful - it never twists or shears vectors. It only stretches, shrinks or flips (vector direction is same just flipped in opposite way).</p>

<p>Moving on we bring our three suspects from the same vector space:</p>
<ul>
  <li>Candidate 1: v1 = [1, 0] -&gt; lies along the x-axis.</li>
  <li>Candidate 2: v2 = [0, 1] -&gt; lies along the y-axis.</li>
  <li>Candidate 3: v3 = [1, 1] -&gt; not algined with any of the matrix‚Äôs axes.</li>
</ul>

<p><img src="/assets/images/matrix_decom_eigen/matrix_decom_eigen_4.png" alt="Matrix transforming candidate vectors" /></p>

<p>We observe the behaviour a <strong>symmetric matrix</strong> exhibits under the local view -</p>

<blockquote>
  <ul>
    <li><strong>Scaling</strong><br />
The matrix keeps the directions but changes the length</li>
    <li><strong>Flipping</strong>
The matrix just flips the vector but direction remains same (negative eiegenvalue)</li>
    <li><strong>Direction change</strong> 
If the matrix changes the direction of the vector, its <strong>not an eigenvector</strong>.</li>
  </ul>
</blockquote>

<p>Our Good Cop is doing great job, but its getting cumbersome, progressing slowly - one vector at a time, though he is also discovering how our <em>symmetric criminal matrix</em> behaves but its exhausting. From the far end of interoggation room, the Bad Cop has been watching, his patience is burning and suddenly he snaps.</p>

<blockquote>
  <p>‚ÄúEnough of this one-vector nonsense! We‚Äôre doing this MY way. Reveal EVERYTHING. All directions and secrets. Right now.‚Äù</p>
</blockquote>

<h2 id="global-view">Global View</h2>

<p>The Bad Cop demands for spilling out the entire structure of the matrix at once - <code class="language-plaintext highlighter-rouge">Diagonalization</code></p>

<p>Instead of asking:</p>
<blockquote>
  <p>‚ÄúHow do you treat this vector?‚Äù‚Äù</p>
</blockquote>

<p>he asks:</p>
<blockquote>
  <p><em>‚ÄúAlong which directions do you <strong>ALWAYS</strong> stretch or <strong>ALWAYS</strong> flip, no matter who you transform?‚Äù</em></p>
</blockquote>

\[A \;=\; 
\underbrace{Q}_{\text{eigenvectors}}
\;
\underbrace{\Lambda}_{\text{eigenvalues}}
\;
\underbrace{Q^{\mathsf{T}}}_{\text{transpose of }Q}\]

<p>This is <strong>diagonalization</strong>, the global view, the full confession and the moment the entire behaviour of the matrix becomes clear at once.</p>

<p>Before a step further into diagonalization, lets first quickly touch upon the idea of <strong>a change of basis</strong>.
Sometimes a matrix looks complicated in our standard coordinate system, but if we use the <strong>same transformation</strong> in a difference coordinate system, the things become much simpler to understand.
Thats the idea behind using a <strong>similarity transformation</strong> - it let‚Äôs us describe the same matrix in an new basis where its behaviour is easier to interpret and work with.</p>

<p>Relating this concept back to our story: as the interrogation continues, the Good Cop slightly changes the environment ‚Äî he walks in with coffee and snacks, hoping to make the criminal matrix a bit more comfortable.
This small shift is our metaphor for a change of basis:
sometimes simply changing the perspective or the coordinate system is enough to make the matrix speak more clearly.</p>

<p>So <em>diagonalization is a special case of similarity transformations</em> and below show the equation breakdown:</p>

\[A \;=\; 
\underbrace{P}_{\text{columns: eigenvectors}}
\;
\underbrace{D}_{\text{diagonal matrix of eigenvalues}}
\;
\underbrace{P^{-1}}_{\text{change-of-basis back}}\]

<p>Lets take another example and will make it confess through digonalization.</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>

<span class="n">A</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span>
              <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">]])</span>
</code></pre></div></div>
<p>on solving the it, we get</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">P</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> 
        <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]])</span>

<span class="n">D</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span>
            <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">]])</span>
</code></pre></div></div>

<p>For easier understanding, the criminal matrix taken is simple, and notice the diagonal eigen values matrix is same as our criminal matrix. Now that the Bad Cop has extracted the full confession,
the Good Cop returns to verify the behavior using two test vectors:</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">vector</span> <span class="mi">1</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>

<span class="n">vector</span> <span class="mi">2</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">([</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
</code></pre></div></div>
<p>Once the transformation is applied to them below we can how its gets transformed</p>
<p align="center">
  <img src="/assets/images/matrix_decom_eigen/matrix_decomp_global_view.png" alt="cops with vector" width="650" />
</p>
<p align="center"><em>Global-view: input vector transformation.</em></p>

<p>All code, diagonalization steps, and visualizations are available in the following notebook: <a href="https://github.com/luaGeeko/the-storyverse-journal/blob/main/Matrix_Decompositions.ipynb">Matrix Decompositions Notebook</a></p>

<p>So far our Cops did a great job in taking out information from a <strong>well-behaved criminal matrix</strong>- a neat, square one that confessed cleanly through eigenvalues and eigenvectors‚Ä¶<strong>but real criminals aren‚Äôt always square.</strong></p>

<p>So what if the matrix isn‚Äôt square at all ?
What if it holds <strong>hidden behaviour that eigen decomposition alone can‚Äôt expose?</strong></p>

<p align="center">
  <img src="/assets/images/matrix_decom_eigen/eigen_decom_svd_init.png" alt="cops with vector" width="650" />
</p>
<p align="center"><em>Next Case: Singular Value Decomposition.</em></p>

<p>In the next part, we need to upgrade our investigation, pushing through will be needed for breaking the criminal. We are going to look into <strong>Singular Value Decomposition(SVD)</strong>- think it as tool our cop detectives are going to use to make criminal confess that refused to be square, simple or honest.</p>

<p>Stay tuned as we open the next case file and bring the entire SVD task force into the interrogation room.</p>

<p>¬© 2025 Shruti Verma</p>

<p>If this article helped you, feel free to cite or share it ‚Äî a small mention goes a long way.
I‚Äôd also love to hear your thoughts, suggestions, or feedback.</p>

<p>You can reach me here:
‚Ä¢ <a href="hop2work@gmail.com">Email</a>
‚Ä¢ <a href="https://www.linkedin.com/in/shruti31">LinkedIn</a></p>]]></content><author><name>LuaGeeko</name></author><category term="linear algebra" /><category term="machine learning" /><summary type="html"><![CDATA[The Good Cop - Bad Cop Routine.]]></summary></entry><entry><title type="html">The untold story of SVM: A Village Divided (Part 1)</title><link href="http://localhost:4000/2025/11/04/The-untold-story-of-SVM.html" rel="alternate" type="text/html" title="The untold story of SVM: A Village Divided (Part 1)" /><published>2025-11-04T15:30:00+05:30</published><updated>2025-11-04T15:30:00+05:30</updated><id>http://localhost:4000/2025/11/04/The-untold-story-of-SVM</id><content type="html" xml:base="http://localhost:4000/2025/11/04/The-untold-story-of-SVM.html"><![CDATA[<blockquote>
  <p><em>This post reimagines the mechanics of Support Vector Machines (SVM) through a story-driven metaphor of a village divided. Beyond the narrative, we dive into the intuition behind linear SVMs ‚Äî how they construct a hyperplane that maximizes the geometric margin between two classes. We explain the roles of support vectors, the functional margin, and how the direction vector w governs the orientation and classification confidence. Whether you‚Äôre a beginner or brushing up on fundamentals, this piece offers a conceptual bridge between storytelling and math.</em></p>
</blockquote>

<p>Once upon a time, there was a village called Ascots, a peaceful village where two groups Orange Ascots and Blue Ascots lived happily. But on one tragic day, a fight broke out and the disagreement sparked a tension that couldn‚Äôt be resolved. They cannot relocate to new place, bloodshed to be avoided and above all the peace had to be restored.</p>

<p>Thus, a decision was made by the village‚Äôs elder Ascot:</p>

<p>A <strong>boundary must be drawn</strong> ‚Äî a fence that would separate the two groups as much as possible to avoid any future conflicts.</p>

<h2 id="not-just-distance-but-confidence">Not Just Distance, But Confidence</h2>

<p>One question now troubled the elder:</p>

<blockquote>
  <p><em><strong>How</strong> should this fence be designed, and <strong>where</strong> exactly should it be built?</em></p>
</blockquote>

<p>Unsure of the answer, he sought the help of a wise problem solver of the land ‚Äî <strong>SVM</strong>. Upon accepting the challenge, SVM went for a walk in the village thinking deeply:</p>

<blockquote>
  <p><em>‚ÄúIf I place a fence, how close will the nearest house be on either side?‚Äù</em></p>
</blockquote>

<p>This thought led to the core idea from a simple concept called the geometric margin :</p>

<ul>
  <li>the actual distance from the fence (hyperplane) to the closest data point (house).</li>
  <li>and we want to maximize this margin ‚ûï.</li>
</ul>

<p>But SVM wasn‚Äôt only thinking about <strong>distance</strong>. <em>He also wanted to be <strong>confident</strong> in placing each villager‚Äôs house on the correct side of the boundary.</em></p>

<ul>
  <li>Positive ‚úÖ if they were on the right side of the fence</li>
  <li>Negative ‚ùå if not</li>
</ul>

<p>This is called the functional margin:</p>

<ul>
  <li>it‚Äôs not the physical distance but a <strong>confidence score</strong> ‚Äî <em>how well the data point (house) was classified and how far (in <strong>sign and scale</strong>) it is from the decision boundary.</em></li>
</ul>

<p><img src="/assets/images/linear_svm/svm_vilage_github1.png" alt="SVM margin illustration" /></p>

<p>SVM realized to build the best fence, he had to consider both <strong>score</strong> and the <strong>distance</strong>. So he worked out on the plan and visited the houses that matter the most ‚Äî the <em>ones closest to the boundary</em>, the ones that would support his decision. He also called upon the elder to discuss his strategy.</p>

<h2 id="the-support-vectors">The Support Vectors</h2>
<p>Ascot the Elder, wanted to understand how the fence was drawn and so SVM explains -</p>

<ul>
  <li>There could be <strong>multiple fences</strong> (decision boundaries) between two groups.</li>
  <li>But the <strong>optimal</strong> one he found, makes sure the <strong>distance between the fence and the house of villagers placed on either side of the line is maximum</strong>. That is why he met some of the villagers, calling them ‚Äî support vectors. The Elder exclaimed:</li>
</ul>

<blockquote>
  <p><em>‚ÄúAh, so the fence is placed based on the closest villagers to ensure the largest gap between the two groups and actually define where the fence goes!‚Äù</em></p>
</blockquote>

<p><img src="/assets/images/linear_svm/svm_village_github2.png" alt="We need Support Vector" /></p>

<h2 id="the-equation-behind-the-fence">The Equation Behind the Fence</h2>
<p>SVM now explains about the functional margin equation.</p>

<ul>
  <li>It represents a <em>hyperplane</em> in geometry ‚Äî a <em>flat surface</em> (like a line in 2D) that separates the space</li>
</ul>

<p>The <em>fence isn‚Äôt just a line</em>:</p>

<ul>
  <li>it has a <strong>direction</strong> (which way it‚Äôs tilted) ‚ÜñÔ∏è</li>
  <li>and a <strong>position</strong> (how high or low it sits) ‚ÜïÔ∏è</li>
</ul>

<p>This is what is called the <strong>functional margin</strong>. It doesn‚Äôt just tell us <em>‚Äòhow far‚Äô</em> ‚Äî it tells us <em>‚Äòhow confident‚Äô</em> we are that each villager is on the correct side. It uses:</p>

<ul>
  <li>the <strong>direction vector</strong> (w) ‚û°Ô∏è.</li>
  <li>and a <strong>bias term (b)</strong> to describe the fence fully.</li>
  <li>The <strong>direction comes from w</strong>: it tells which way the fence tilts.</li>
  <li>The <strong>position is adjusted by b</strong>: it shifts the fence up or down.</li>
</ul>

\[w^T x + b = 0\]

<figure style="text-align:center;">
  <img src="/assets/images/linear_svm/svm_village_github4.png" alt="Hyperplane Equation xkcd" width="500" />
  <figcaption><em>Hyperplane Equation Guides.</em></figcaption>
</figure>

<h2 id="-svm-continues-to-explain">üí¨ SVM continues to explain:</h2>
<blockquote>
  <p><em>‚ÄúImagine each villager‚Äôs home is a point with two coordinates (x‚ÇÅ, x‚ÇÇ).
When I multiply that by a direction vector (w‚ÇÅ, w‚ÇÇ), I‚Äôm checking how well the house aligns with that direction.
So now, if the result is positive as per the hyperplane equation, they are to be placed on the positive class ‚Äî otherwise on the negative class side.‚Äù</em></p>
</blockquote>

<p>Elder looked a bit confused ü§î</p>

<blockquote>
  <p><em>‚ÄúHmm‚Ä¶ makes sense, but how do you decide which side is for positive class and which is negative?‚Äù</em></p>
</blockquote>

\[y =
\begin{cases}
+1, &amp; \text{if } w^T x + b &gt; 0 \\
-1, &amp; \text{if } w^T x + b &lt; 0
\end{cases}\]

<figure style="text-align:center;">
  <img src="/assets/images/linear_svm/svm_village_github5.png" alt="Door fence xkcd" width="500" />
  <figcaption><em>Door in the fence.</em></figcaption>
</figure>

<p>SVM explained, with simple analogy of <strong>door-in-the-fence</strong>, <em>‚ÄúImagine this fence has door in it and the door swings open in the direction of <strong>w</strong>. The side it opens towards is the positive class. The side it swings away from is the negative class.‚Äù</em></p>

<p>Lets try to understand concepts till here by taking small data and applying linear svm using scikit-learn.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">svm</span>

<span class="c1"># lets take some sample points for 2 classes
</span><span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">([</span>
    <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span>  <span class="c1"># Class +1
</span>    <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span>
    <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span>
    <span class="p">[</span><span class="mi">6</span><span class="p">,</span> <span class="mi">5</span><span class="p">],</span>  <span class="c1"># Class -1
</span>    <span class="p">[</span><span class="mi">7</span><span class="p">,</span> <span class="mi">8</span><span class="p">],</span>
    <span class="p">[</span><span class="mi">8</span><span class="p">,</span> <span class="mi">6</span><span class="p">]</span>
<span class="p">])</span>

<span class="c1"># define true labels
</span><span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">])</span>

<span class="n">clf</span> <span class="o">=</span> <span class="n">svm</span><span class="p">.</span><span class="n">SVC</span><span class="p">(</span><span class="n">kernel</span><span class="o">=</span><span class="s">'linear'</span><span class="p">)</span>
<span class="c1"># fit to our sample data
</span><span class="n">clf</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>

<span class="c1"># extracting w and b
</span><span class="n">w</span> <span class="o">=</span> <span class="n">clf</span><span class="p">.</span><span class="n">coef_</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">b</span> <span class="o">=</span> <span class="n">clf</span><span class="p">.</span><span class="n">intercept_</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

</code></pre></div></div>

<p>Here we get <strong>w: [-0.33333333 -0.33333333]</strong>, where w[0] corresponds to x-axis (think of 1st feature) and w[1] corresponds to y-axis (2nd feature). Next let‚Äôs try plotting the points with decision boundary and margin lines.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<span class="n">plt</span><span class="p">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="n">y</span><span class="o">==</span><span class="mi">1</span><span class="p">][:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[</span><span class="n">y</span><span class="o">==</span><span class="mi">1</span><span class="p">][:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="s">'orange'</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">'Class +1 (Orange Ascots)'</span><span class="p">,</span> <span class="n">edgecolors</span><span class="o">=</span><span class="s">'k'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="n">y</span><span class="o">==-</span><span class="mi">1</span><span class="p">][:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[</span><span class="n">y</span><span class="o">==-</span><span class="mi">1</span><span class="p">][:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="s">'blue'</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">'Class -1 (Blue Ascots)'</span><span class="p">,</span> <span class="n">edgecolors</span><span class="o">=</span><span class="s">'k'</span><span class="p">)</span>

<span class="c1"># for decision boundary - smooth continuous line lets take points from linspace
</span><span class="n">x_vals</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>
<span class="c1"># compute decsion boundary values
</span><span class="n">y_vals</span> <span class="o">=</span> <span class="o">-</span><span class="p">(</span><span class="n">w</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="n">x_vals</span> <span class="o">+</span> <span class="n">b</span><span class="p">)</span> <span class="o">/</span> <span class="n">w</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
<span class="c1"># compute margin distance from descision boundary to support vectors
</span><span class="n">margin</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">/</span> <span class="n">np</span><span class="p">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nb">sum</span><span class="p">(</span><span class="n">w</span> <span class="o">**</span> <span class="mi">2</span><span class="p">))</span>
<span class="c1"># we will margin on both the sides 
</span><span class="n">y_margin_pos</span> <span class="o">=</span> <span class="n">y_vals</span> <span class="o">+</span> <span class="n">margin</span>
<span class="n">y_margin_neg</span> <span class="o">=</span> <span class="n">y_vals</span> <span class="o">-</span> <span class="n">margin</span>

<span class="c1"># now lets plot them
</span><span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_vals</span><span class="p">,</span> <span class="n">y_vals</span><span class="p">,</span> <span class="s">'k-'</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">'Decision Boundary'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_vals</span><span class="p">,</span> <span class="n">y_margin_pos</span><span class="p">,</span> <span class="s">'k--'</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">'Margin (+1)'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_vals</span><span class="p">,</span> <span class="n">y_margin_neg</span><span class="p">,</span> <span class="s">'k--'</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">'Margin (-1)'</span><span class="p">)</span>

<span class="c1"># legend and labels
</span><span class="n">plt</span><span class="p">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="p">.</span><span class="n">title</span><span class="p">(</span><span class="s">"Linear SVM with Decision Boundary and Margins"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s">"Feature 1"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s">"Feature 2"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">grid</span><span class="p">(</span><span class="bp">True</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>

</code></pre></div></div>

<p>Now SVM wants us to try out a new point [2.5, 2.5] and see to what class it gets assigned to.s</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># lets test a new point
</span><span class="n">test_point</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">([</span><span class="mf">2.5</span><span class="p">,</span> <span class="mf">2.5</span><span class="p">])</span>
<span class="c1"># our decision boundary equation
</span><span class="n">svm_score</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">dot</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">test_point</span><span class="p">)</span> <span class="o">+</span> <span class="n">b</span>
<span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"Functional margin: </span><span class="si">{</span><span class="n">svm_score</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>
</code></pre></div></div>

<p>The gives out svm score as 1 and gets class output of +1.</p>

<p>Elder asks <em>‚ÄúOh, interesting ‚Äî that makes sense now! But when you explained the direction of w with the door-in-the-fence analogy, what if <strong>w</strong> points in the opposite direction? Does that mean the door opens the other way ‚Äî and the classes get switched?‚Äù</em></p>

<p>SVM replies <em>‚ÄúExactly! When w points in the opposite direction, the decision boundary stays the same, but the classification flips ‚Äî what was once the positive side becomes negative, and vice versa.‚Äù The below plot shows how flipping w and b changes <strong>classification results</strong>, even though the geometric boundary doesn‚Äôt move.</em></p>

<p><img src="/assets/images/linear_svm/svm_village_github6.png" alt="Decision Boundary w and b" /></p>

<p>You can find the proper code for the above here <a href="https://github.com/luaGeeko/the-storyverse-journal/blob/main/SVM_tutorial.ipynb">SVM notebook</a></p>

<p>Now wrapping up Part 1, where we laid the foundation by understanding the <strong>functional margin</strong> and <strong>geometric margin</strong> ‚Äî two crucial concepts that help us measure how well our Support Vector Machine (SVM) separates classes with maximum confidence. In the next part, we‚Äôll take this a step further and explore how to <strong>optimize</strong> the decision boundary to, introduce the role of constraints in this optimization, and uncover the difference between <strong>hard and soft margins</strong> ‚Äî essential ideas in SVMs.</p>

<p>¬© 2025 Shruti Verma</p>

<p>If this article helped you, feel free to cite or share it ‚Äî a small mention goes a long way.
I‚Äôd also love to hear your thoughts, suggestions, or feedback.</p>

<p>You can reach me here:
‚Ä¢ <a href="hop2work@gmail.com">Email</a>
‚Ä¢ <a href="https://www.linkedin.com/in/shruti31">LinkedIn</a></p>]]></content><author><name>LuaGeeko</name></author><category term="svm" /><category term="machine learning" /><summary type="html"><![CDATA[A village split in two, a line drawn with purpose‚Äîwelcome to the world of SVMs.]]></summary></entry></feed>